"""
æŠ–éŸ³ï¼ˆä¸­å›½ç‰ˆï¼‰ä¸‹è½½å™¨
ä½¿ç”¨ç³»ç»Ÿæµè§ˆå™¨ï¼ˆEdge/Chromeï¼‰å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå¯åŠ¨å‰å°†ç³»ç»Ÿ profile çš„ cookies åŒæ­¥åˆ°ç‹¬ç«‹ç›®å½•ï¼Œ
é¿å…ä¸æ­£åœ¨è¿è¡Œçš„æµè§ˆå™¨äº‰æŠ¢æ–‡ä»¶é”ã€‚
"""

import asyncio
import re
import shutil
import time
import json
import os
import random
from pathlib import Path
from typing import AsyncGenerator, Optional
from datetime import datetime

import httpx

from src.core.models import Platform, VideoInfo, DownloadResult, DownloadProgress
from src.core.interfaces import IDownloader, IProgressCallback
from src.core.exceptions import DownloaderError, VideoNotFoundError, NetworkError
from src.config import get_settings


class DouyinDownloader(IDownloader):
    """æŠ–éŸ³è§†é¢‘ä¸‹è½½å™¨ - ä½¿ç”¨ç³»ç»Ÿæµè§ˆå™¨ï¼ˆEdge/Chromeï¼‰å¯æ‰§è¡Œæ–‡ä»¶"""

    # ç‹¬ç«‹ profile ç›®å½•ï¼Œé¿å…ä¸æ­£åœ¨è¿è¡Œçš„ç³»ç»Ÿæµè§ˆå™¨äº‰æŠ¢æ–‡ä»¶é”
    PROFILE_DIR = Path(os.path.expandvars(r"%LocalAppData%\video-analysis\chrome-profile"))

    # æ—¥å¿—ç›®å½• - ä¿å­˜è°ƒè¯•ä¿¡æ¯
    LOG_DIR = Path(__file__).parent.parent.parent.parent / "logs"

    # ========== åçˆ¬é…ç½® ==========
    # æœ€å¤§å•æ¬¡å»¶è¿Ÿ 2.5 ç§’
    SCROLL_DELAY = (1.0, 2.0)       # æ»šåŠ¨é—´éš”ï¼ˆç§’ï¼‰
    SCROLL_RETRY_DELAY = (5.0, 8.0)  # æ— æ–°å†…å®¹æ—¶çš„é‡è¯•ç­‰å¾…ï¼ˆç§’ï¼‰ï¼Œé¡µé¢åŠ è½½æ…¢æ—¶éœ€è¦æ›´ä¹…
    PAGE_LOAD_DELAY = (1.5, 2.5)    # é¡µé¢åŠ è½½åç­‰å¾…ï¼ˆç§’ï¼‰
    VIDEO_INTERVAL = (0.8, 1.8)     # è§†é¢‘ä¹‹é—´é—´éš”ï¼ˆç§’ï¼‰
    DOWNLOAD_INTERVAL = (0.3, 1.0)     # ä¸‹è½½é—´éš”ï¼ˆç§’ï¼‰

    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    ]

    @staticmethod
    def _random_delay(delay_range: tuple) -> int:
        """ç”Ÿæˆéšæœºå»¶è¿Ÿæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        return int(random.uniform(delay_range[0], delay_range[1]) * 1000)

    @staticmethod
    async def _check_captcha(page) -> tuple[bool, str]:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦å‡ºç°éªŒè¯ç æˆ–ç™»å½•å¼¹çª—

        Returns:
            (æ˜¯å¦æœ‰éªŒè¯ç /ç™»å½•, ç±»å‹æè¿°)
        """
        # æŒ‰ä¼˜å…ˆçº§æ£€æµ‹
        checks = [
            # ç™»å½•å¼¹çª— - ä¼˜å…ˆæ£€æµ‹
            ('div[class*="login-panel"]', 'ç™»å½•å¼¹çª—'),
            ('div[class*="loginContainer"]', 'ç™»å½•å¼¹çª—'),
            ('div[class*="login-guide"]', 'ç™»å½•å¼¹çª—'),
            ('div.login-mask', 'ç™»å½•å¼¹çª—'),
            # éªŒè¯ç 
            ('div.captcha_verify_container', 'æ»‘å—éªŒè¯ç '),
            ('div[class*="captcha-verify"]', 'æ»‘å—éªŒè¯ç '),
            ('div#captcha_container', 'éªŒè¯ç '),
            ('div.verify-captcha-container', 'å›¾ç‰‡éªŒè¯ç '),
            ('div[class*="secsdk-captcha"]', 'å®‰å…¨éªŒè¯ç '),
            ('div[class*="captcha"]', 'éªŒè¯ç '),
            ('iframe[src*="captcha"]', 'éªŒè¯ç '),
            # æµ·å¤–è®¿é—®æç¤º
            ('div[class*="region"]', 'åœ°åŒºé™åˆ¶æç¤º'),
        ]
        for selector, block_type in checks:
            try:
                elem = await page.query_selector(selector)
                if elem and await elem.is_visible():
                    return True, block_type
            except Exception:
                pass
        return False, ""

    async def _wait_for_auth_resolved(self, page, max_wait: int = 120) -> bool:
        """
        ç­‰å¾…ç”¨æˆ·å®ŒæˆéªŒè¯ç /ç™»å½•ï¼Œæœ€å¤šç­‰å¾… max_wait ç§’
        æ¯æ¬¡æ£€æµ‹åˆ°é˜»æ–­ä¼šè®°å½•æ—¥å¿—
        """
        start = time.time()
        last_type = ""
        while time.time() - start < max_wait:
            has_block, block_type = await self._check_captcha(page)
            if not has_block:
                return True
            # ç±»å‹å˜åŒ–æ—¶è®°å½•æ—¥å¿—
            if block_type != last_type:
                print(f"[æŠ–éŸ³ä¸‹è½½] â³ ç­‰å¾…ç”¨æˆ·å®Œæˆ: {block_type}")
                last_type = block_type
            await page.wait_for_timeout(1000)
        return False

    async def _wait_and_retry_auth(self, page, max_retries: int = 10) -> bool:
        """
        å¾ªç¯æ£€æµ‹éªŒè¯ç /ç™»å½•ï¼Œç›´åˆ°é¡µé¢æ­£å¸¸æˆ–è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
        éªŒè¯ç å’Œç™»å½•å¯èƒ½åå¤å‡ºç°ï¼Œæ¯æ¬¡æ£€æµ‹åˆ°ç­‰å¾… 10-12 ç§’åé‡è¯•

        Returns:
            True å¦‚æœæœ€ç»ˆé€šè¿‡ï¼ŒFalse å¦‚æœè¶…æ—¶
        """
        for attempt in range(max_retries):
            has_block, block_type = await self._check_captcha(page)

            if not has_block:
                if attempt > 0:
                    print(f"[æŠ–éŸ³ä¸‹è½½] âœ“ éªŒè¯/ç™»å½•å·²å…¨éƒ¨å®Œæˆ")
                return True

            # æ£€æµ‹åˆ°é˜»æ–­ï¼Œè®°å½•å¹¶ç­‰å¾…ç”¨æˆ·å¤„ç†
            print(f"[æŠ–éŸ³ä¸‹è½½] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ... (ç¬¬{attempt+1}æ¬¡)")

            # ç­‰å¾…ç”¨æˆ·å®Œæˆï¼ˆæœ€å¤š120ç§’ï¼‰
            resolved = await self._wait_for_auth_resolved(page, 120)
            if not resolved:
                print(f"[æŠ–éŸ³ä¸‹è½½] âœ— ç­‰å¾…è¶…æ—¶")
                return False

            print(f"[æŠ–éŸ³ä¸‹è½½] âœ“ {block_type} å·²é€šè¿‡")

            # ç­‰å¾… 10-12 ç§’è®©é¡µé¢åˆ·æ–°ï¼Œå¯èƒ½è¿˜æœ‰ä¸‹ä¸€ä¸ªéªŒè¯
            wait_time = random.uniform(10, 12)
            print(f"[æŠ–éŸ³ä¸‹è½½] ç­‰å¾…é¡µé¢åˆ·æ–° ({wait_time:.1f}s)...")
            await page.wait_for_timeout(int(wait_time * 1000))

        return False

    async def _save_debug_info(self, page, reason: str = "unknown") -> None:
        """ä¿å­˜è°ƒè¯•ä¿¡æ¯ï¼šæˆªå›¾ + é¡µé¢æºä»£ç """
        timestamp = int(time.time())

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        self.LOG_DIR.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æˆªå›¾
        screenshot_path = self.LOG_DIR / f"debug_{reason}_{timestamp}.png"
        try:
            await page.screenshot(path=str(screenshot_path))
            print(f"[æŠ–éŸ³ä¸‹è½½] ğŸ“¸ å·²ä¿å­˜æˆªå›¾: {screenshot_path}")
        except Exception as e:
            print(f"[æŠ–éŸ³ä¸‹è½½] æˆªå›¾ä¿å­˜å¤±è´¥: {e}")

        # ä¿å­˜é¡µé¢æºä»£ç 
        html_path = self.LOG_DIR / f"debug_{reason}_{timestamp}.html"
        try:
            content = await page.content()
            with open(html_path, "w", encoding="utf-8") as f:
                f.write(content)
            print(f"[æŠ–éŸ³ä¸‹è½½] ğŸ“„ å·²ä¿å­˜æºä»£ç : {html_path}")
        except Exception as e:
            print(f"[æŠ–éŸ³ä¸‹è½½] æºä»£ç ä¿å­˜å¤±è´¥: {e}")

        # æ‰“å°é¡µé¢åŸºæœ¬ä¿¡æ¯
        try:
            title = await page.title()
            current_url = page.url
            print(f"[æŠ–éŸ³ä¸‹è½½] é¡µé¢æ ‡é¢˜: {title}")
            print(f"[æŠ–éŸ³ä¸‹è½½] å½“å‰URL: {current_url}")
        except Exception:
            pass

    def _save_video_urls_log(self, user_url: str, video_urls: list[str]) -> Path:
        """
        ä¿å­˜æå–çš„è§†é¢‘URLåˆ—è¡¨åˆ°è°ƒè¯•æ—¥å¿—æ–‡ä»¶

        Args:
            user_url: ç”¨æˆ·ä¸»é¡µURL
            video_urls: æå–çš„è§†é¢‘URLåˆ—è¡¨

        Returns:
            æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_path = self.LOG_DIR / f"video_urls_{timestamp}.txt"

        try:
            self.LOG_DIR.mkdir(parents=True, exist_ok=True)
            with open(log_path, "w", encoding="utf-8") as f:
                f.write(f"# æŠ–éŸ³è§†é¢‘URLæå–æ—¥å¿—\n")
                f.write(f"# æ—¶é—´: {datetime.now().isoformat()}\n")
                f.write(f"# ç”¨æˆ·ä¸»é¡µ: {user_url}\n")
                f.write(f"# è§†é¢‘æ•°é‡: {len(video_urls)}\n")
                f.write(f"# {'='*50}\n\n")

                for i, url in enumerate(video_urls, 1):
                    f.write(f"{i:03d}. {url}\n")

            print(f"[æŠ–éŸ³ä¸‹è½½] ğŸ“ å·²ä¿å­˜è§†é¢‘URLåˆ—è¡¨: {log_path}")
            return log_path
        except Exception as e:
            print(f"[æŠ–éŸ³ä¸‹è½½] âš ï¸ ä¿å­˜URLåˆ—è¡¨å¤±è´¥: {e}")
            return log_path

    def __init__(self):
        self.settings = get_settings()
        self._progress_callback: Optional[IProgressCallback] = None
        self._current_progress = DownloadProgress()
        self.PROFILE_DIR.mkdir(parents=True, exist_ok=True)

    @property
    def platform(self) -> Platform:
        return Platform.DOUYIN

    @property
    def supported_domains(self) -> list[str]:
        return [
            "douyin.com",
            "v.douyin.com",
            "iesdouyin.com",
        ]

    # æŠ–éŸ³æœ‰æ•ˆé“¾æ¥æ­£åˆ™
    VIDEO_URL_PATTERN = re.compile(r'douyin\.com/video/\d+')
    USER_URL_PATTERN = re.compile(r'douyin\.com/user/[A-Za-z0-9_-]+')
    SHORT_URL_PATTERN = re.compile(r'v\.douyin\.com/[A-Za-z0-9]+')

    @staticmethod
    def is_user_profile_url(url: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç”¨æˆ·ä¸»é¡µURL"""
        return "/user/" in url

    @classmethod
    def validate_url(cls, url: str) -> tuple[bool, str]:
        """
        éªŒè¯æŠ–éŸ³é“¾æ¥æ ¼å¼æ˜¯å¦æœ‰æ•ˆ

        Returns:
            (is_valid, error_message)
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºæŠ–éŸ³åŸŸå
        if not any(domain in url.lower() for domain in ["douyin.com", "iesdouyin.com"]):
            return False, "ä¸æ˜¯æŠ–éŸ³é“¾æ¥"

        # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ ¼å¼
        if cls.VIDEO_URL_PATTERN.search(url):
            return True, ""
        if cls.USER_URL_PATTERN.search(url):
            return True, ""
        if cls.SHORT_URL_PATTERN.search(url):
            return True, ""

        # æ— æ•ˆæ ¼å¼ï¼Œç»™å‡ºæç¤º
        return False, (
            "æŠ–éŸ³é“¾æ¥æ ¼å¼ä¸æ­£ç¡®ã€‚æ”¯æŒçš„æ ¼å¼ï¼š\n"
            "  â€¢ è§†é¢‘é“¾æ¥: https://www.douyin.com/video/7456789012345678901\n"
            "  â€¢ ç”¨æˆ·ä¸»é¡µ: https://www.douyin.com/user/MS4wLjABAAAAxxxxx\n"
            "  â€¢ çŸ­é“¾æ¥: https://v.douyin.com/xxxxxx\n"
            "å½“å‰é“¾æ¥ä¸ç¬¦åˆä»¥ä¸Šæ ¼å¼ï¼Œè¯·æ£€æŸ¥åé‡è¯•ã€‚"
        )

    def supports_url(self, url: str) -> bool:
        url_lower = url.lower()
        return any(domain in url_lower for domain in self.supported_domains)

    def _extract_video_id(self, url: str) -> str:
        match = re.search(r'/video/(\d+)', url)
        if match:
            return match.group(1)
        match = re.search(r'modal_id=(\d+)', url)
        if match:
            return match.group(1)
        match = re.search(r'/(\d{15,20})', url)
        if match:
            return match.group(1)
        return ""

    def _get_chrome_path(self) -> Optional[str]:
        """è·å–æœ¬åœ° Chrome/Edge æµè§ˆå™¨è·¯å¾„ï¼ˆä¼˜å…ˆ Chromeï¼‰"""
        possible_paths = [
            # Chrome ä¼˜å…ˆ
            os.path.expandvars(r"%ProgramFiles%\Google\Chrome\Application\chrome.exe"),
            os.path.expandvars(r"%ProgramFiles(x86)%\Google\Chrome\Application\chrome.exe"),
            os.path.expandvars(r"%LocalAppData%\Google\Chrome\Application\chrome.exe"),
            # Edge å¤‡ç”¨
            os.path.expandvars(r"%ProgramFiles%\Microsoft\Edge\Application\msedge.exe"),
            os.path.expandvars(r"%ProgramFiles(x86)%\Microsoft\Edge\Application\msedge.exe"),
        ]
        for path in possible_paths:
            if os.path.exists(path):
                return path
        return None

    def _get_native_user_data_dir(self) -> Optional[Path]:
        """è·å–ç³»ç»Ÿæµè§ˆå™¨çš„ç”¨æˆ·æ•°æ®ç›®å½•ï¼ˆä¼˜å…ˆ Chromeï¼Œä¸ _get_chrome_path ä¿æŒä¸€è‡´ï¼‰"""
        candidates = [
            # Chrome ä¼˜å…ˆ
            Path(os.path.expandvars(r"%LocalAppData%\Google\Chrome\User Data")),
            # Edge å¤‡ç”¨
            Path(os.path.expandvars(r"%LocalAppData%\Microsoft\Edge\User Data")),
        ]
        for p in candidates:
            if (p / "Default").exists():
                return p
        return None

    def _is_profile_initialized(self) -> bool:
        """æ£€æŸ¥ Profile æ˜¯å¦å·²åˆå§‹åŒ–ï¼ˆå·²æœ‰ç™»å½•æ€ï¼‰"""
        cookies_file = self.PROFILE_DIR / "Default" / "Network" / "Cookies"
        local_state = self.PROFILE_DIR / "Local State"
        return cookies_file.exists() and local_state.exists()

    def _sync_native_profile(self, force: bool = False) -> None:
        """
        å°†ç³»ç»Ÿæµè§ˆå™¨ï¼ˆEdge/Chromeï¼‰çš„é…ç½®ã€ç™»å½•æ€ã€ç½‘ç«™ç¼“å­˜åŒæ­¥åˆ°ç‹¬ç«‹ profile ç›®å½•ã€‚

        Args:
            force: æ˜¯å¦å¼ºåˆ¶åŒæ­¥ï¼ˆè¦†ç›–å·²æœ‰æ•°æ®ï¼‰

        æ³¨æ„ï¼šåªåœ¨é¦–æ¬¡æˆ–å¼ºåˆ¶æ—¶åŒæ­¥ï¼Œé¿å…è¦†ç›–å·²æœ‰ç™»å½•çŠ¶æ€ã€‚
        """
        # å¦‚æœå·²åˆå§‹åŒ–ä¸”éå¼ºåˆ¶ï¼Œè·³è¿‡åŒæ­¥
        if self._is_profile_initialized() and not force:
            print(f"[æŠ–éŸ³ä¸‹è½½] Profile å·²å­˜åœ¨ï¼Œè·³è¿‡åŒæ­¥ï¼ˆä¿ç•™å·²æœ‰ç™»å½•æ€ï¼‰")
            return

        native_dir = self._get_native_user_data_dir()
        if not native_dir:
            print(f"[æŠ–éŸ³ä¸‹è½½] æœªæ‰¾åˆ°ç³»ç»Ÿæµè§ˆå™¨ Profileï¼Œå°†ä½¿ç”¨ç©ºç™½é…ç½®")
            return

        target = self.PROFILE_DIR
        print(f"[æŠ–éŸ³ä¸‹è½½] é¦–æ¬¡åŒæ­¥ï¼Œä» {native_dir} å¤åˆ¶ç™»å½•æ€...")

        # å•ä¸ªæ–‡ä»¶ï¼šcookie åŠ å¯†å¯†é’¥ + cookie æ•°æ®åº“ + é…ç½®
        files = [
            "Local State",
            "Default/Network/Cookies",
            "Default/Network/Cookies-journal",
            "Default/Cookies",
            "Default/Cookies-journal",
            "Default/Preferences",
            "Default/Secure Preferences",
            "Default/Web Data",          # è‡ªåŠ¨å¡«å……ã€æœç´¢å¼•æ“ç­‰é…ç½®
            "Default/Web Data-journal",
            "Default/Network/Trust Tokens",
        ]
        # ç›®å½•ï¼šç™»å½•æ€å­˜å‚¨ + ç½‘ç«™ç¼“å­˜
        dirs = [
            "Default/Local Storage",
            "Default/Session Storage",
            "Default/IndexedDB",         # IndexedDB æ•°æ®ï¼ˆéƒ¨åˆ†ç½‘ç«™ç™»å½•æ€å­˜è¿™é‡Œï¼‰
            "Default/Cache",             # HTTP ç¼“å­˜
            "Default/Code Cache",        # JS/WASM ç¼–è¯‘ç¼“å­˜
            "Default/Service Worker",    # Service Worker æ³¨å†ŒåŠç¼“å­˜
            "Default/Network",           # ç½‘ç»œçŠ¶æ€ï¼ˆHSTSã€DNS ç¼“å­˜ç­‰ï¼‰
        ]

        for rel in files:
            src = native_dir / rel
            dst = target / rel
            if src.exists():
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    shutil.copy2(str(src), str(dst))
                except (PermissionError, OSError):
                    pass  # æµè§ˆå™¨è¿è¡Œä¸­å¯èƒ½é”å®šéƒ¨åˆ†æ–‡ä»¶ï¼Œè·³è¿‡

        for rel in dirs:
            src = native_dir / rel
            dst = target / rel
            if src.exists():
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    if dst.exists():
                        shutil.rmtree(str(dst), ignore_errors=True)
                    shutil.copytree(str(src), str(dst), dirs_exist_ok=True,
                                    ignore_dangling_symlinks=True)
                except (PermissionError, OSError):
                    pass

    async def _get_video_data_playwright(self, url: str) -> dict:
        """ä½¿ç”¨ç³»ç»Ÿæµè§ˆå™¨å¯æ‰§è¡Œæ–‡ä»¶å¯åŠ¨ Playwrightï¼Œè·å–è§†é¢‘æ•°æ®"""
        try:
            from playwright.async_api import async_playwright
        except ImportError:
            raise DownloaderError(url=url, message="è¯·å®‰è£… playwright: pip install playwright")

        # å°†ç³»ç»Ÿæµè§ˆå™¨çš„ç™»å½•æ€åŒæ­¥åˆ°ç‹¬ç«‹ profileï¼ˆä»…é¦–æ¬¡ï¼‰
        self._sync_native_profile()

        video_data = {}
        chrome_path = self._get_chrome_path()
        print(f"[æŠ–éŸ³ä¸‹è½½] ä½¿ç”¨æµè§ˆå™¨: {chrome_path or 'Playwright å†…ç½®'}")

        async with async_playwright() as p:
            launch_options = {
                "user_data_dir": str(self.PROFILE_DIR),
                "headless": False,  # ä½¿ç”¨å¯è§æµè§ˆå™¨ï¼Œä¸æ˜“è¢«æ£€æµ‹
                "args": [
                    "--disable-infobars",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-extensions",
                ],
                "viewport": {"width": 1280, "height": 800},
                "ignore_default_args": ["--enable-automation", "--no-sandbox"],
            }

            # å¦‚æœæ‰¾åˆ°æœ¬åœ° Chromeï¼Œä½¿ç”¨å®ƒ
            if chrome_path:
                launch_options["executable_path"] = chrome_path

            print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            context = await p.chromium.launch_persistent_context(**launch_options)
            print(f"[æŠ–éŸ³ä¸‹è½½] æµè§ˆå™¨å·²å¯åŠ¨")

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                await page.add_init_script("""
                    // éšè— webdriver æ ‡è¯†
                    Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                    delete navigator.__proto__.webdriver;

                    // ä¼ªé€ æ’ä»¶
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [
                            { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer' },
                            { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai' },
                            { name: 'Native Client', filename: 'internal-nacl-plugin' }
                        ]
                    });

                    // è¯­è¨€è®¾ç½®
                    Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });

                    // Chrome å¯¹è±¡
                    window.chrome = { runtime: {}, loadTimes: () => {}, csi: () => {} };

                    // éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
                    Object.defineProperty(navigator, 'maxTouchPoints', { get: () => 1 });
                    Object.defineProperty(navigator, 'hardwareConcurrency', { get: () => 8 });
                """)

                # ç›‘å¬ç½‘ç»œè¯·æ±‚ï¼Œæ•è·è§†é¢‘ä¿¡æ¯ API
                video_info_captured = asyncio.Event()

                async def handle_response(response):
                    nonlocal video_data
                    try:
                        resp_url = response.url
                        if "aweme/v1/web/aweme/detail" in resp_url or "/aweme/detail" in resp_url:
                            if response.status == 200:
                                try:
                                    data = await response.json()
                                    if data.get("aweme_detail"):
                                        video_data = data["aweme_detail"]
                                        video_info_captured.set()
                                except:
                                    pass
                    except:
                        pass

                page.on("response", handle_response)

                # è®¿é—®è§†é¢‘é¡µé¢
                print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨è®¿é—®é¡µé¢: {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=60000)
                print(f"[æŠ–éŸ³ä¸‹è½½] é¡µé¢å·²åŠ è½½ï¼Œç­‰å¾…è§†é¢‘ä¿¡æ¯...")

                # å¾ªç¯æ£€æµ‹éªŒè¯ç /ç™»å½•ï¼ˆå¯èƒ½åå¤å‡ºç°ï¼‰
                auth_ok = await self._wait_and_retry_auth(page, max_retries=10)
                if not auth_ok:
                    await self._save_debug_info(page, "video_auth_timeout")
                    raise DownloaderError(url=url, message="éªŒè¯ç /ç™»å½•è¶…æ—¶æœªå®Œæˆ")

                # ç­‰å¾…è§†é¢‘ä¿¡æ¯è¢«æ•è·
                try:
                    await asyncio.wait_for(video_info_captured.wait(), timeout=20)
                    print(f"[æŠ–éŸ³ä¸‹è½½] å·²æ•è·è§†é¢‘ä¿¡æ¯")
                except asyncio.TimeoutError:
                    print(f"[æŠ–éŸ³ä¸‹è½½] ç­‰å¾…è¶…æ—¶ï¼Œå°è¯•ä»é¡µé¢æå–...")

                # å¦‚æœæ²¡æ•è·åˆ°ï¼Œå°è¯•ä»é¡µé¢æå–
                if not video_data:
                    await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))
                    video_data = await self._extract_from_page(page)
                    if video_data:
                        print(f"[æŠ–éŸ³ä¸‹è½½] ä»é¡µé¢æå–åˆ°è§†é¢‘ä¿¡æ¯")

            finally:
                print(f"[æŠ–éŸ³ä¸‹è½½] å…³é—­æµè§ˆå™¨...")
                await context.close()

        if not video_data:
            raise VideoNotFoundError(url, "æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ã€‚å¦‚æœæ˜¯é¦–æ¬¡ä½¿ç”¨ï¼Œæµè§ˆå™¨çª—å£å¯èƒ½éœ€è¦ä½ å®ŒæˆéªŒè¯ã€‚")

        return video_data

    async def _extract_from_page(self, page) -> dict:
        """ä»é¡µé¢æå–è§†é¢‘æ•°æ®"""
        # å°è¯•ä» RENDER_DATA æå–
        render_data = await page.evaluate('''() => {
            const script = document.getElementById('RENDER_DATA');
            if (script) {
                try {
                    return decodeURIComponent(script.textContent);
                } catch {
                    return script.textContent;
                }
            }
            return null;
        }''')

        if render_data:
            try:
                data = json.loads(render_data)
                for key, value in data.items():
                    if isinstance(value, dict):
                        if "aweme" in value:
                            aweme = value.get("aweme", {})
                            if "detail" in aweme:
                                return aweme["detail"]
                        if "video" in value and "author" in value:
                            return value
            except:
                pass

        # å°è¯•ä» __INITIAL_STATE__ æå–
        initial_state = await page.evaluate('''() => {
            if (window.__INITIAL_STATE__) {
                return JSON.stringify(window.__INITIAL_STATE__);
            }
            return null;
        }''')

        if initial_state:
            try:
                data = json.loads(initial_state)
                if "aweme" in data:
                    return data["aweme"]
            except:
                pass

        return {}

    def _extract_video_url(self, video_data: dict) -> str:
        """ä»è§†é¢‘æ•°æ®ä¸­æå–ä¸‹è½½URL"""
        video = video_data.get("video", {})

        # æ–¹æ³•1: play_addr
        play_addr = video.get("play_addr", {})
        url_list = play_addr.get("url_list", [])
        if url_list:
            video_url = url_list[0]
            video_url = video_url.replace("playwm", "play")
            return video_url

        # æ–¹æ³•2: bit_rate (é€‰æ‹©æœ€é«˜ç ç‡)
        bit_rate = video.get("bit_rate", [])
        if bit_rate:
            sorted_rates = sorted(bit_rate, key=lambda x: x.get("bit_rate", 0), reverse=True)
            play_addr = sorted_rates[0].get("play_addr", {})
            url_list = play_addr.get("url_list", [])
            if url_list:
                return url_list[0]

        # æ–¹æ³•3: download_addr
        download_addr = video.get("download_addr", {})
        url_list = download_addr.get("url_list", [])
        if url_list:
            return url_list[0]

        raise DownloaderError(url="", message="æ— æ³•è·å–è§†é¢‘ä¸‹è½½é“¾æ¥")

    def _parse_video_info(self, video_data: dict, url: str) -> VideoInfo:
        """è§£æè§†é¢‘ä¿¡æ¯"""
        author_info = video_data.get("author", {})
        statistics = video_data.get("statistics", {})

        upload_date = None
        if create_time := video_data.get("create_time"):
            try:
                upload_date = datetime.fromtimestamp(create_time)
            except:
                pass

        duration = video_data.get("video", {}).get("duration", 0)
        if duration > 1000:
            duration = duration // 1000

        return VideoInfo(
            url=url,
            platform=Platform.DOUYIN,
            video_id=video_data.get("aweme_id", self._extract_video_id(url)),
            title=video_data.get("desc", "æŠ–éŸ³è§†é¢‘") or "æŠ–éŸ³è§†é¢‘",
            author=author_info.get("nickname"),
            duration=duration,
            thumbnail=video_data.get("video", {}).get("cover", {}).get("url_list", [None])[0],
            description=video_data.get("desc"),
            upload_date=upload_date,
            view_count=statistics.get("play_count"),
            like_count=statistics.get("digg_count"),
            available_qualities=["best"],
            raw_data=video_data,
        )

    async def get_video_info(self, url: str) -> VideoInfo:
        """è·å–è§†é¢‘ä¿¡æ¯"""
        # éªŒè¯é“¾æ¥æ ¼å¼
        is_valid, error_msg = self.validate_url(url)
        if not is_valid:
            raise DownloaderError(url=url, message=error_msg)

        video_data = await self._get_video_data_playwright(url)
        return self._parse_video_info(video_data, url)

    async def download(
        self,
        url: str,
        output_dir: Path,
        quality: str = "best",
        progress_callback: Optional[IProgressCallback] = None,
    ) -> DownloadResult:
        """ä¸‹è½½è§†é¢‘"""
        # éªŒè¯é“¾æ¥æ ¼å¼
        is_valid, error_msg = self.validate_url(url)
        if not is_valid:
            raise DownloaderError(url=url, message=error_msg)

        self._progress_callback = progress_callback
        self._current_progress = DownloadProgress()
        start_time = time.time()

        try:
            video_data = await self._get_video_data_playwright(url)
            video_info = self._parse_video_info(video_data, url)

            if progress_callback:
                progress_callback.on_start(video_info)

            download_url = self._extract_video_url(video_data)

            safe_title = re.sub(r'[\s\\/*?:"<>|]', "_", video_info.title).strip("_")[:80]
            if not safe_title.strip():
                safe_title = f"douyin_{video_info.video_id}"
            file_path = output_dir / f"{safe_title}.mp4"

            await self._download_file(download_url, file_path, progress_callback)

            file_size = file_path.stat().st_size if file_path.exists() else None

            result = DownloadResult(
                success=True,
                video_info=video_info,
                file_path=file_path,
                file_size=file_size,
                elapsed_time=time.time() - start_time,
            )

            if progress_callback:
                progress_callback.on_complete(result)

            return result

        except (VideoNotFoundError, DownloaderError):
            raise
        except Exception as e:
            result = DownloadResult(
                success=False,
                video_info=VideoInfo(
                    url=url,
                    platform=Platform.DOUYIN,
                    video_id=self._extract_video_id(url),
                    title="æœªçŸ¥",
                ),
                error_message=str(e),
                elapsed_time=time.time() - start_time,
            )

            if progress_callback:
                progress_callback.on_error(e)

            return result

    async def _download_file(
        self,
        url: str,
        file_path: Path,
        progress_callback: Optional[IProgressCallback] = None,
    ) -> None:
        """ä¸‹è½½æ–‡ä»¶"""
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.douyin.com/",
        }

        async with httpx.AsyncClient(headers=headers, follow_redirects=True, timeout=120) as client:
            async with client.stream("GET", url) as response:
                response.raise_for_status()

                total_size = int(response.headers.get("content-length", 0))
                downloaded = 0

                with open(file_path, "wb") as f:
                    async for chunk in response.aiter_bytes(chunk_size=65536):
                        f.write(chunk)
                        downloaded += len(chunk)

                        if progress_callback and total_size:
                            progress = DownloadProgress(
                                downloaded_bytes=downloaded,
                                total_bytes=total_size,
                                percentage=(downloaded / total_size) * 100,
                                status="downloading",
                            )
                            progress_callback.on_progress(progress)

    @staticmethod
    def _format_size(size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size < 1024:
            return f"{size} B"
        elif size < 1024 * 1024:
            return f"{size / 1024:.1f} KB"
        elif size < 1024 * 1024 * 1024:
            return f"{size / (1024 * 1024):.1f} MB"
        else:
            return f"{size / (1024 * 1024 * 1024):.2f} GB"

    @classmethod
    def _get_random_ua(cls) -> str:
        return random.choice(cls.USER_AGENTS)

    async def _download_file_http(self, download_url: str, file_path: Path) -> tuple[bool, int, str]:
        """ç”¨ HTTP ä¸‹è½½è§†é¢‘æ–‡ä»¶ï¼Œè¿”å› (success, file_size, error_message)"""
        headers = {
            "User-Agent": self._get_random_ua(),
            "Referer": "https://www.douyin.com/",
            "Accept": "video/webm,video/ogg,video/*;q=0.9,application/ogg;q=0.7,audio/*;q=0.6,*/*;q=0.5",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        }
        try:
            async with httpx.AsyncClient(headers=headers, follow_redirects=True, timeout=180) as client:
                async with client.stream("GET", download_url) as response:
                    response.raise_for_status()
                    total_size = int(response.headers.get("content-length", 0))
                    downloaded = 0
                    with open(file_path, "wb") as f:
                        async for chunk in response.aiter_bytes(chunk_size=65536):
                            f.write(chunk)
                            downloaded += len(chunk)
                            if total_size > 0:
                                pct = downloaded / total_size * 100
                                print(f"\r[ä¸‹è½½è¿›åº¦] {pct:.1f}% ({self._format_size(downloaded)}/{self._format_size(total_size)})", end="", flush=True)
                    print()
                    return True, file_path.stat().st_size, ""
        except Exception as e:
            return False, 0, str(e)

    async def _download_subtitle_from_aweme(self, aweme_detail: dict, srt_path: Path) -> bool:
        """ä» aweme_detail ä¸­æå–å­—å¹•å¹¶ä¿å­˜ä¸º SRT æ–‡ä»¶"""
        subtitle_url = None
        for field in ["video_subtitle", "caption_infos"]:
            items = aweme_detail.get(field)
            if not items or not isinstance(items, list):
                continue
            for item in items:
                url = item.get("Url") or item.get("url") or item.get("subtitle_url")
                if url:
                    subtitle_url = url
                    break
            if subtitle_url:
                break

        if not subtitle_url:
            return False

        try:
            headers = {
                "User-Agent": self._get_random_ua(),
                "Referer": "https://www.douyin.com/",
            }
            async with httpx.AsyncClient(headers=headers, follow_redirects=True, timeout=30) as client:
                resp = await client.get(subtitle_url)
                resp.raise_for_status()
                content = resp.text
                if content.strip():
                    with open(srt_path, "w", encoding="utf-8") as f:
                        f.write(content)
                    return True
        except Exception as e:
            print(f"[å­—å¹•] ä¸‹è½½å¤±è´¥: {e}")
        return False

    async def _extract_username_from_page(self, page) -> str:
        """ä»æŠ–éŸ³ç”¨æˆ·ä¸»é¡µæå–ç”¨æˆ·å"""
        try:
            title = await page.title()
            if title and "æŠ–éŸ³" in title:
                name = title.replace("çš„ä¸»é¡µ - æŠ–éŸ³", "").replace("çš„æŠ–éŸ³", "").strip()
                if name:
                    return name
            name_el = await page.query_selector('h1[class*="name"], span[class*="nickname"], [data-e2e="user-info-nickname"]')
            if name_el:
                name = await name_el.text_content()
                if name:
                    return name.strip()
        except Exception:
            pass
        return ""

    @staticmethod
    def _get_existing_videos(folder: Path) -> set[str]:
        """è·å–æ–‡ä»¶å¤¹ä¸­å·²ä¸‹è½½çš„è§†é¢‘URLï¼ˆé€šè¿‡è¯»å–å…ƒæ•°æ®ï¼‰"""
        existing = set()
        metadata_file = folder / "_metadata.json"
        if metadata_file.exists():
            try:
                with open(metadata_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    for video in data.get("downloaded_videos", []):
                        if video.get("url"):
                            existing.add(video["url"])
            except Exception:
                pass
        return existing

    @staticmethod
    def _save_user_metadata(folder: Path, user_url: str, user_info: dict, videos: list[dict]):
        """ä¿å­˜ç”¨æˆ·ä¸‹è½½å…ƒæ•°æ®åˆ°æ–‡ä»¶å¤¹"""
        metadata = {
            "user_url": user_url,
            "username": user_info.get("username", ""),
            "work_count": user_info.get("work_count", 0),
            "video_count": user_info.get("video_count", 0),
            "non_video_count": user_info.get("non_video_count", 0),
            "last_updated": datetime.now().isoformat(),
            "downloaded_videos": videos,
        }
        metadata_file = folder / "_metadata.json"
        with open(metadata_file, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        urls_file = folder / "_video_urls.txt"
        with open(urls_file, "w", encoding="utf-8") as f:
            f.write(f"# ç”¨æˆ·: {user_info.get('username', 'æœªçŸ¥')}\n")
            f.write(f"# ä¸»é¡µ: {user_url}\n")
            f.write(f"# ä½œå“æ•°: {user_info.get('work_count', 0)} | è§†é¢‘æ•°: {user_info.get('video_count', 0)}\n")
            f.write(f"# æ›´æ–°æ—¶é—´: {datetime.now().isoformat()}\n")
            f.write(f"# {'='*50}\n\n")
            for i, video in enumerate(videos, 1):
                status = "âœ“" if video.get("success") else "âœ—"
                f.write(f"{i:03d}. [{status}] {video.get('url', '')}\n")
                if video.get("title"):
                    f.write(f"     æ ‡é¢˜: {video['title']}\n")

    async def _check_login_status(self, page) -> bool:
        """æ£€æŸ¥æŠ–éŸ³ç™»å½•çŠ¶æ€"""
        try:
            login_btn = await page.query_selector(
                'button:has-text("ç™»å½•"), a:has-text("ç™»å½•"), '
                'button:has-text("Login"), a:has-text("Login"), '
                'div[class*="login-btn"], button[class*="login"], '
                'div[class*="login-guide"]'
            )
            if login_btn and await login_btn.is_visible():
                return False
            avatar = await page.query_selector(
                'img[class*="avatar"], div[class*="avatar"], '
                'img[class*="Avatar"], div[class*="Avatar"]'
            )
            if avatar and await avatar.is_visible():
                return True
            return True
        except Exception:
            return True

    async def download_user_videos_stream(
        self,
        user_url: str,
        output_dir: Path,
        quality: str = "best",
        max_retries: int = 3,
    ) -> AsyncGenerator[dict, None]:
        """
        æµå¼ä¸‹è½½æŠ–éŸ³ç”¨æˆ·ä¸»é¡µè§†é¢‘ï¼Œé€ä¸ª yield äº‹ä»¶ã€‚

        ç‰¹æ€§ï¼š
        - æµè§ˆå™¨ä¿æŒæ‰“å¼€çŠ¶æ€ï¼Œä¾›ä¸‹æ¬¡å¤ç”¨
        - è‡ªåŠ¨åˆ›å»ºä»¥ç”¨æˆ·åå‘½åçš„æ–‡ä»¶å¤¹
        - è·³è¿‡å·²ä¸‹è½½çš„è§†é¢‘
        - ä¿å­˜å…ƒæ•°æ®ï¼ˆè§†é¢‘URLåˆ—è¡¨ã€ç”¨æˆ·ä¿¡æ¯ï¼‰
        - åŒºåˆ†ä½œå“æ•°å’Œè§†é¢‘æ•°
        """
        from src.core.events import (
            make_extracting_event, make_extracted_event,
            make_downloading_event, make_downloaded_event,
            make_retrying_event, make_done_event, make_error_event,
        )
        from src.services.browser_manager import get_browser_manager

        start_time = time.time()

        # çŠ¶æ€å˜é‡
        succeeded_count = 0
        skipped_count = 0
        failed_list: list[dict] = []
        non_video_list: list[dict] = []
        video_urls: list[str] = []
        downloaded_urls: set[str] = set()
        work_count = 0
        video_count = 0
        username = ""
        user_folder: Optional[Path] = None
        max_retry_rounds = max_retries

        print(f"\n{'='*60}")
        print(f"[ç”¨æˆ·ä¸»é¡µä¸‹è½½] å¼€å§‹å¤„ç†: {user_url}")
        print(f"{'='*60}\n")

        yield make_extracting_event("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")

        self._sync_native_profile()
        chrome_path = self._get_chrome_path()

        browser_manager = await get_browser_manager()
        page = await browser_manager.get_page(self.PROFILE_DIR, chrome_path)

        try:
            # ========== ç¬¬ä¸€æ­¥ï¼šè®¿é—®ç”¨æˆ·ä¸»é¡µ ==========
            print(f"[æ­¥éª¤1] æ­£åœ¨è®¿é—®ç”¨æˆ·ä¸»é¡µ...")
            yield make_extracting_event("æ­£åœ¨è®¿é—®ç”¨æˆ·ä¸»é¡µï¼Œæå–è§†é¢‘åˆ—è¡¨...")

            await page.goto(user_url, wait_until="domcontentloaded", timeout=60000)
            await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

            # å¾ªç¯æ£€æµ‹éªŒè¯ç /ç™»å½•
            for auth_attempt in range(10):
                has_block, block_type = await self._check_captcha(page)
                if not has_block:
                    if auth_attempt > 0:
                        print(f"[ä¿¡æ¯] âœ“ éªŒè¯/ç™»å½•å·²å…¨éƒ¨å®Œæˆ")
                    break
                print(f"[è­¦å‘Š] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ... (ç¬¬{auth_attempt+1}æ¬¡)")
                yield make_extracting_event(f"æ£€æµ‹åˆ°{block_type}ï¼Œè¯·åœ¨æµè§ˆå™¨ä¸­å®ŒæˆéªŒè¯...")
                resolved = await self._wait_for_auth_resolved(page, 120)
                if not resolved:
                    await self._save_debug_info(page, "auth_timeout")
                    yield make_error_event(f"éªŒè¯ç /ç™»å½•è¶…æ—¶æœªå®Œæˆ")
                    return
                print(f"[ä¿¡æ¯] âœ“ {block_type} å·²é€šè¿‡")
                await page.wait_for_timeout(int(random.uniform(10, 12) * 1000))
            else:
                await self._save_debug_info(page, "max_auth_retries")
                yield make_error_event("éªŒè¯/ç™»å½•é‡è¯•æ¬¡æ•°è¿‡å¤š")
                return

            # ç­‰å¾…è§†é¢‘é“¾æ¥
            video_links_found = False
            for load_attempt in range(5):
                print(f"[æ­¥éª¤1] ç­‰å¾…é¡µé¢åŠ è½½... (ç¬¬{load_attempt+1}æ¬¡)")
                try:
                    await page.wait_for_load_state("networkidle", timeout=30000)
                except Exception:
                    pass

                has_block, block_type = await self._check_captcha(page)
                if has_block:
                    print(f"[è­¦å‘Š] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ...")
                    resolved = await self._wait_for_auth_resolved(page, 120)
                    if not resolved:
                        yield make_error_event(f"éªŒè¯ç /ç™»å½•è¶…æ—¶æœªå®Œæˆ")
                        return
                    await page.wait_for_timeout(int(random.uniform(10, 12) * 1000))
                    continue

                try:
                    await page.wait_for_selector('a[href*="/video/"]', timeout=15000)
                    print(f"[æ­¥éª¤1] âœ“ è§†é¢‘é“¾æ¥å·²åŠ è½½")
                    is_logged_in = await self._check_login_status(page)
                    if not is_logged_in:
                        print(f"[è­¦å‘Š] âš ï¸ æŠ–éŸ³æœªç™»å½•ï¼æœªç™»å½•çŠ¶æ€ä¸‹å¯èƒ½æ— æ³•è·å–å…¨éƒ¨è§†é¢‘")
                        yield make_extracting_event("âš ï¸ æœªç™»å½•æŠ–éŸ³ï¼Œå¯èƒ½æ— æ³•è·å–å…¨éƒ¨è§†é¢‘ã€‚å»ºè®®ç™»å½•åé‡è¯•ã€‚")
                        await page.wait_for_timeout(3000)
                    video_links_found = True
                    break
                except Exception:
                    await page.wait_for_timeout(5000)

            if not video_links_found:
                print(f"[è­¦å‘Š] åˆ·æ–°é¡µé¢é‡è¯•...")
                await page.reload(wait_until="networkidle", timeout=60000)
                await page.wait_for_timeout(8000)
                try:
                    await page.wait_for_selector('a[href*="/video/"]', timeout=30000)
                    video_links_found = True
                except Exception:
                    await self._save_debug_info(page, "no_videos_after_refresh")
                    yield make_error_event("æ— æ³•åŠ è½½è§†é¢‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é‡æ–°ç™»å½•")
                    return

            # ========== æå–ç”¨æˆ·åå¹¶åˆ›å»ºæ–‡ä»¶å¤¹ ==========
            username = await self._extract_username_from_page(page)
            if username:
                safe_username = re.sub(r'[\\/*?:"<>|]', "_", username).strip("_")[:50]
                user_folder = output_dir / safe_username
            else:
                user_folder = output_dir / f"douyin_user_{int(time.time())}"

            user_folder.mkdir(parents=True, exist_ok=True)
            print(f"[ç”¨æˆ·ä¸»é¡µä¸‹è½½] ç”¨æˆ·å: {username or 'æœªçŸ¥'}")
            print(f"[ç”¨æˆ·ä¸»é¡µä¸‹è½½] ä¿å­˜ç›®å½•: {user_folder}")

            downloaded_urls = self._get_existing_videos(user_folder)
            if downloaded_urls:
                print(f"[ä¿¡æ¯] å‘ç° {len(downloaded_urls)} ä¸ªå·²ä¸‹è½½çš„è§†é¢‘ï¼Œå°†è·³è¿‡")

            await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

            # ========== æå–ä½œå“æ•°å’Œè§†é¢‘é“¾æ¥ ==========
            extract_js = '''() => {
                const containers = document.querySelectorAll('div[class*="userNewUi"]');
                const links = new Set();
                containers.forEach(container => {
                    const aTags = container.querySelectorAll('a[href]');
                    aTags.forEach(a => {
                        if (a.closest('.user-page-footer')) return;
                        const href = a.getAttribute('href');
                        if (href && href.includes('/video/')) links.add(href);
                    });
                });
                return Array.from(links);
            }'''

            # è·å–ä½œå“æ€»æ•°
            try:
                work_count = await page.evaluate('''() => {
                    const tabs = document.querySelectorAll('span, div');
                    for (const el of tabs) {
                        const text = el.textContent || '';
                        const match = text.match(/ä½œå“[\\s]*([0-9]+)/);
                        if (match) return parseInt(match[1]);
                    }
                    return 0;
                }''') or 0
                if work_count:
                    print(f"[æ­¥éª¤1] é¡µé¢æ˜¾ç¤ºè¯¥ç”¨æˆ·æœ‰ {work_count} ä¸ªä½œå“")
            except Exception:
                work_count = 0

            # æ»šåŠ¨å‰ï¼šä¿å­˜é¡µé¢å¿«ç…§ + æ£€æµ‹å¯æ»šåŠ¨å…ƒç´ 
            await self._save_debug_info(page, "before_scroll")
            try:
                scroll_debug = await page.evaluate('''() => {
                    const results = [];
                    const candidates = [
                        document.scrollingElement,
                        document.documentElement,
                        document.body,
                        ...document.querySelectorAll('[class*="user-tab-content"]'),
                        ...document.querySelectorAll('div[class*="userNewUi"]'),
                        ...document.querySelectorAll('[class*="container"]'),
                        ...document.querySelectorAll('main'),
                    ];
                    for (const el of candidates) {
                        if (!el) continue;
                        const tag = el.tagName || 'unknown';
                        const cls = (el.className || '').toString().substring(0, 80);
                        const sh = el.scrollHeight;
                        const ch = el.clientHeight;
                        const st = el.scrollTop;
                        const ov = getComputedStyle(el).overflow + '/' + getComputedStyle(el).overflowY;
                        if (sh > ch + 10) {
                            results.push(`SCROLLABLE ${tag} cls="${cls}" scrollH=${sh} clientH=${ch} scrollTop=${st} overflow=${ov}`);
                        } else {
                            results.push(`NOT-SCROLLABLE ${tag} cls="${cls}" scrollH=${sh} clientH=${ch} overflow=${ov}`);
                        }
                    }
                    return results;
                }''')
                for line in scroll_debug:
                    print(f"[æ»šåŠ¨è°ƒè¯•] {line}")
            except Exception as e:
                print(f"[æ»šåŠ¨è°ƒè¯•] æ£€æµ‹å¤±è´¥: {e}")

            # æ»šåŠ¨åŠ è½½
            print(f"[æ­¥éª¤1] æ­£åœ¨æ»šåŠ¨åŠ è½½è§†é¢‘åˆ—è¡¨...")
            prev_count = 0
            no_change_rounds = 0

            for i in range(100):
                try:
                    hrefs = await page.evaluate(extract_js)
                except Exception:
                    await page.wait_for_timeout(self._random_delay((0.8, 1.5)))
                    continue

                current_count = len(hrefs)

                if work_count and current_count >= work_count:
                    print(f"[æ­¥éª¤1] âœ“ å·²åŠ è½½å…¨éƒ¨ {current_count}/{work_count} ä¸ªä½œå“é“¾æ¥")
                    break

                if current_count != prev_count:
                    print(f"[æ­¥éª¤1] å·²å‘ç° {current_count}/{work_count or '?'} ä¸ªè§†é¢‘é“¾æ¥...")
                    no_change_rounds = 0
                else:
                    no_change_rounds += 1
                    if no_change_rounds >= 5:
                        if work_count and current_count < work_count:
                            print(f"[è­¦å‘Š] æ»šåŠ¨åæ— æ³•åŠ è½½æ›´å¤šè§†é¢‘ï¼ˆå¯èƒ½éœ€è¦ç™»å½•ï¼‰")
                        break
                    print(f"[æ­¥éª¤1] æœªå‘ç°æ–°å†…å®¹ï¼Œç­‰å¾…é¡µé¢åŠ è½½ ({no_change_rounds}/5)...")
                    await page.wait_for_timeout(self._random_delay(self.SCROLL_RETRY_DELAY))
                    continue
                prev_count = current_count

                await page.mouse.move(random.randint(900, 1100), random.randint(550, 700))
                delta_y = random.randint(600, 1800)
                await page.mouse.wheel(0, delta_y)
                await page.wait_for_timeout(self._random_delay(self.SCROLL_DELAY))

            # æå–é“¾æ¥
            hrefs = await page.evaluate(extract_js)
            for href in hrefs:
                if href.startswith('/video/'):
                    video_urls.append(f"https://www.douyin.com{href}")
                elif 'douyin.com/video/' in href:
                    video_urls.append(href)

            video_count = len(video_urls)
            non_video_count = work_count - video_count if work_count > video_count else 0

            print(f"\n[æ­¥éª¤1] å®Œæˆï¼")
            print(f"  - ä½œå“æ€»æ•°: {work_count}")
            print(f"  - è§†é¢‘æ•°é‡: {video_count}")
            if non_video_count > 0:
                print(f"  - éè§†é¢‘ä½œå“: {non_video_count} (å›¾æ–‡ç­‰)")

            if video_count == 0:
                await self._save_debug_info(page, "no_videos")
                yield make_error_event("æœªæ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œå¯èƒ½éœ€è¦ç™»å½•æˆ–å®ŒæˆéªŒè¯")
                return

            yield make_extracted_event(
                total=video_count,
                work_count=work_count,
                non_video_count=non_video_count,
                message=f"æ‰¾åˆ° {video_count} ä¸ªè§†é¢‘ï¼ˆä½œå“ {work_count} ä¸ªï¼‰ï¼Œå¼€å§‹ä¸‹è½½...",
            )

            # ========== ç¬¬äºŒæ­¥ï¼šé€ä¸ªä¸‹è½½è§†é¢‘ ==========
            print(f"\n[æ­¥éª¤2] å¼€å§‹ä¸‹è½½è§†é¢‘...")
            downloaded_videos_info: list[dict] = []

            for idx, video_url in enumerate(video_urls, 1):
                # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
                if video_url in downloaded_urls:
                    skipped_count += 1
                    print(f"[è§†é¢‘ {idx}/{video_count}] å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    downloaded_videos_info.append({"url": video_url, "title": "", "success": True, "skipped": True})
                    yield make_downloaded_event(
                        index=idx, total=video_count, title="(å·²å­˜åœ¨)",
                        success=True, skipped=True,
                        succeeded_so_far=succeeded_count, skipped_count=skipped_count,
                    )
                    continue

                print(f"\n{'â”€'*50}")
                print(f"[è§†é¢‘ {idx}/{video_count}] {video_url}")

                yield make_downloading_event(
                    index=idx, total=video_count, url=video_url,
                    title=f"è§†é¢‘ {idx}",
                    succeeded_so_far=succeeded_count,
                    remaining=video_count - succeeded_count - len(failed_list) - skipped_count,
                )

                video_data = {}
                video_captured = asyncio.Event()

                async def handle_response(response):
                    nonlocal video_data
                    try:
                        if "aweme/v1/web/aweme/detail" in response.url or "/aweme/detail" in response.url:
                            if response.status == 200:
                                data = await response.json()
                                if data.get("aweme_detail"):
                                    video_data = data["aweme_detail"]
                                    video_captured.set()
                    except Exception:
                        pass

                page.on("response", handle_response)

                try:
                    print(f"[è§†é¢‘ {idx}/{video_count}] æ­£åœ¨è·å–ä¸‹è½½åœ°å€...")
                    await page.goto(video_url, wait_until="domcontentloaded", timeout=30000)
                    await page.wait_for_timeout(self._random_delay((1.0, 2.0)))

                    has_block, block_type = await self._check_captcha(page)
                    if has_block:
                        print(f"[è§†é¢‘ {idx}/{video_count}] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼")
                        resolved = await self._wait_for_auth_resolved(page, 120)
                        if not resolved:
                            failed_list.append({"url": video_url, "title": f"è§†é¢‘ {idx}", "error": "éªŒè¯è¶…æ—¶"})
                            yield make_downloaded_event(
                                index=idx, total=video_count, title=f"è§†é¢‘ {idx}",
                                success=False, error="éªŒè¯ç è¶…æ—¶", permanently_failed=True,
                            )
                            page.remove_listener("response", handle_response)
                            continue
                        await page.wait_for_timeout(int(random.uniform(10, 12) * 1000))

                    try:
                        await asyncio.wait_for(video_captured.wait(), timeout=15)
                    except asyncio.TimeoutError:
                        print(f"[è§†é¢‘ {idx}/{video_count}] è·å–è¶…æ—¶ï¼Œå°è¯•ä»é¡µé¢æå–...")

                    page.remove_listener("response", handle_response)

                    if not video_data:
                        video_data = await self._extract_from_page(page)

                    if not video_data:
                        print(f"[è§†é¢‘ {idx}/{video_count}] âœ— æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
                        failed_list.append({"url": video_url, "title": f"è§†é¢‘ {idx}", "error": "æ— æ³•è·å–è§†é¢‘ä¿¡æ¯"})
                        yield make_downloaded_event(
                            index=idx, total=video_count, title=f"è§†é¢‘ {idx}",
                            success=False, error="æ— æ³•è·å–è§†é¢‘ä¿¡æ¯", permanently_failed=True,
                        )
                        continue

                    title = video_data.get("desc", f"è§†é¢‘ {idx}") or f"è§†é¢‘ {idx}"

                    # æå–ä¸‹è½½åœ°å€ (å¤ç”¨å·²æœ‰æ–¹æ³•)
                    try:
                        download_url = self._extract_video_url(video_data)
                    except DownloaderError:
                        download_url = None

                    if not download_url:
                        print(f"[è§†é¢‘ {idx}/{video_count}] âœ— æ— æ³•è·å–ä¸‹è½½åœ°å€")
                        failed_list.append({"url": video_url, "title": title, "error": "æ— æ³•è·å–ä¸‹è½½åœ°å€"})
                        yield make_downloaded_event(
                            index=idx, total=video_count, title=title,
                            success=False, error="æ— æ³•è·å–ä¸‹è½½åœ°å€", permanently_failed=True,
                        )
                        continue

                    # ä»URLæå–è§†é¢‘IDï¼Œç”¨äºæ–‡ä»¶åå»é‡
                    video_id_match = re.search(r'/video/(\d+)', video_url)
                    video_id = video_id_match.group(1)[-8:] if video_id_match else ""

                    safe_title = re.sub(r'[\s\\/*?:"<>|]', "_", title).strip("_")[:80]
                    if not safe_title.strip():
                        safe_title = f"douyin_{idx}"
                    filename = f"{safe_title}_{video_id}.mp4" if video_id else f"{safe_title}.mp4"
                    file_path = user_folder / filename

                    old_file_path = user_folder / f"{safe_title}.mp4"

                    if file_path.exists() or (old_file_path.exists() and old_file_path != file_path):
                        existing = file_path if file_path.exists() else old_file_path
                        print(f"[è§†é¢‘ {idx}/{video_count}] æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {existing.name}")
                        skipped_count += 1
                        downloaded_urls.add(video_url)
                        downloaded_videos_info.append({"url": video_url, "title": title, "success": True, "skipped": True, "file_path": str(existing)})
                        yield make_downloaded_event(
                            index=idx, total=video_count, title=title,
                            success=True, skipped=True, file_path=str(existing),
                        )
                        continue

                    print(f"[è§†é¢‘ {idx}/{video_count}] æ­£åœ¨ä¸‹è½½: {title[:30]}...")
                    success, file_size, error_msg = await self._download_file_http(download_url, file_path)

                    if success:
                        succeeded_count += 1
                        downloaded_urls.add(video_url)
                        downloaded_videos_info.append({"url": video_url, "title": title, "success": True, "file_path": str(file_path)})
                        print(f"[è§†é¢‘ {idx}/{video_count}] âœ“ ä¸‹è½½æˆåŠŸ: {self._format_size(file_size)}")

                        srt_path = file_path.with_suffix(".srt")
                        has_subtitle = await self._download_subtitle_from_aweme(video_data, srt_path)
                        if has_subtitle:
                            print(f"[è§†é¢‘ {idx}/{video_count}] âœ“ å­—å¹•å·²ä¿å­˜: {srt_path.name}")

                        yield make_downloaded_event(
                            index=idx, total=video_count, title=title,
                            success=True, file_path=str(file_path),
                            file_size_human=self._format_size(file_size),
                            has_subtitle=has_subtitle,
                            succeeded_so_far=succeeded_count,
                            remaining=video_count - succeeded_count - len(failed_list) - skipped_count,
                        )
                        await asyncio.sleep(self._random_delay(self.DOWNLOAD_INTERVAL) / 1000)
                    else:
                        print(f"[è§†é¢‘ {idx}/{video_count}] âœ— ä¸‹è½½å¤±è´¥: {error_msg}")
                        failed_list.append({"url": video_url, "title": title, "error": error_msg})
                        downloaded_videos_info.append({"url": video_url, "title": title, "success": False, "error": error_msg})
                        yield make_downloaded_event(
                            index=idx, total=video_count, title=title,
                            success=False, error=error_msg, permanently_failed=True,
                        )

                except Exception as e:
                    print(f"[è§†é¢‘ {idx}/{video_count}] âœ— å¼‚å¸¸: {str(e)}")
                    page.remove_listener("response", handle_response)
                    failed_list.append({"url": video_url, "title": f"è§†é¢‘ {idx}", "error": str(e)})
                    yield make_downloaded_event(
                        index=idx, total=video_count, title=f"è§†é¢‘ {idx}",
                        success=False, error=str(e), permanently_failed=True,
                    )

                await page.wait_for_timeout(self._random_delay(self.VIDEO_INTERVAL))

            # ========== ç¬¬ä¸‰æ­¥ï¼šå¤±è´¥è§†é¢‘é‡è¯• ==========
            retry_round = 0
            while failed_list and retry_round < max_retry_rounds:
                retry_round += 1
                failed_urls = [f["url"] for f in failed_list]
                retry_count = len(failed_urls)

                print(f"\n{'='*60}")
                print(f"[é‡è¯• ç¬¬{retry_round}/{max_retry_rounds}è½®] æœ‰ {retry_count} ä¸ªè§†é¢‘ä¸‹è½½å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•...")
                print(f"{'='*60}")

                yield make_retrying_event(
                    round_num=retry_round, max_rounds=max_retry_rounds,
                    failed_count=retry_count,
                )

                # å›åˆ°ç”¨æˆ·é¦–é¡µé‡æ–°è·å–é“¾æ¥
                print(f"[é‡è¯•] å›åˆ°ç”¨æˆ·é¦–é¡µé‡æ–°è·å–è§†é¢‘é“¾æ¥...")
                await page.goto(user_url, wait_until="domcontentloaded", timeout=60000)
                await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

                has_block, block_type = await self._check_captcha(page)
                if has_block:
                    print(f"[é‡è¯•] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ...")
                    resolved = await self._wait_for_auth_resolved(page, 120)
                    if not resolved:
                        print(f"[é‡è¯•] éªŒè¯è¶…æ—¶ï¼Œè·³è¿‡æœ¬è½®é‡è¯•")
                        break
                    await page.wait_for_timeout(int(random.uniform(5, 8) * 1000))

                try:
                    await page.wait_for_selector('a[href*="/video/"]', timeout=15000)
                except Exception:
                    print(f"[é‡è¯•] æ— æ³•åŠ è½½è§†é¢‘åˆ—è¡¨ï¼Œè·³è¿‡æœ¬è½®é‡è¯•")
                    break

                # æ»šåŠ¨åŠ è½½æ‰€æœ‰è§†é¢‘
                print(f"[é‡è¯•] æ»šåŠ¨åŠ è½½è§†é¢‘åˆ—è¡¨...")
                for _ in range(30):
                    await page.mouse.move(random.randint(900, 1100), random.randint(550, 700))
                    await page.mouse.wheel(0, random.randint(600, 1800))
                    await page.wait_for_timeout(self._random_delay(self.SCROLL_DELAY))
                    hrefs = await page.evaluate(extract_js)
                    if work_count and len(hrefs) >= work_count:
                        break

                old_failed_list = failed_list.copy()
                failed_list.clear()

                for idx, failed_item in enumerate(old_failed_list, 1):
                    video_url = failed_item["url"]

                    print(f"\n{'â”€'*50}")
                    print(f"[é‡è¯• {idx}/{retry_count}] {video_url}")

                    yield make_downloading_event(
                        index=idx, total=retry_count, url=video_url,
                        title=failed_item.get("title", f"è§†é¢‘ {idx}"),
                        is_retry=True, retry_round=retry_round,
                    )

                    video_data = {}
                    video_captured = asyncio.Event()

                    async def handle_response_retry(response):
                        nonlocal video_data
                        try:
                            if "aweme/v1/web/aweme/detail" in response.url or "/aweme/detail" in response.url:
                                if response.status == 200:
                                    data = await response.json()
                                    if data.get("aweme_detail"):
                                        video_data = data["aweme_detail"]
                                        video_captured.set()
                        except Exception:
                            pass

                    page.on("response", handle_response_retry)

                    try:
                        await page.goto(video_url, wait_until="domcontentloaded", timeout=30000)
                        await page.wait_for_timeout(self._random_delay((1.5, 2.5)))

                        has_block, block_type = await self._check_captcha(page)
                        if has_block:
                            print(f"[é‡è¯• {idx}/{retry_count}] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼")
                            resolved = await self._wait_for_auth_resolved(page, 120)
                            if not resolved:
                                failed_list.append(failed_item)
                                page.remove_listener("response", handle_response_retry)
                                continue
                            await page.wait_for_timeout(int(random.uniform(5, 8) * 1000))

                        try:
                            await asyncio.wait_for(video_captured.wait(), timeout=15)
                        except asyncio.TimeoutError:
                            pass

                        page.remove_listener("response", handle_response_retry)

                        if not video_data:
                            video_data = await self._extract_from_page(page)

                        if not video_data:
                            print(f"[é‡è¯• {idx}/{retry_count}] âœ— ä»æ— æ³•è·å–è§†é¢‘ä¿¡æ¯")
                            failed_list.append(failed_item)
                            continue

                        title = video_data.get("desc", failed_item.get("title", f"è§†é¢‘ {idx}")) or f"è§†é¢‘ {idx}"

                        try:
                            download_url = self._extract_video_url(video_data)
                        except DownloaderError:
                            download_url = None

                        if not download_url:
                            print(f"[é‡è¯• {idx}/{retry_count}] âœ— ä»æ— æ³•è·å–ä¸‹è½½åœ°å€")
                            failed_list.append({"url": video_url, "title": title, "error": "æ— æ³•è·å–ä¸‹è½½åœ°å€"})
                            continue

                        safe_title = re.sub(r'[\s\\/*?:"<>|]', "_", title).strip("_")[:80]
                        if not safe_title.strip():
                            safe_title = f"douyin_retry_{idx}"
                        file_path = user_folder / f"{safe_title}.mp4"

                        print(f"[é‡è¯• {idx}/{retry_count}] æ­£åœ¨ä¸‹è½½: {title[:30]}...")
                        success, file_size, error_msg = await self._download_file_http(download_url, file_path)

                        if success:
                            succeeded_count += 1
                            downloaded_urls.add(video_url)
                            for v in downloaded_videos_info:
                                if v.get("url") == video_url:
                                    v["success"] = True
                                    v["file_path"] = str(file_path)
                                    v.pop("error", None)
                                    break
                            else:
                                downloaded_videos_info.append({"url": video_url, "title": title, "success": True, "file_path": str(file_path)})

                            print(f"[é‡è¯• {idx}/{retry_count}] âœ“ é‡è¯•æˆåŠŸ: {self._format_size(file_size)}")
                            yield make_downloaded_event(
                                index=idx, total=retry_count, title=title,
                                success=True, file_path=str(file_path),
                                file_size_human=self._format_size(file_size),
                                is_retry=True, retry_round=retry_round,
                            )
                        else:
                            print(f"[é‡è¯• {idx}/{retry_count}] âœ— é‡è¯•ä»å¤±è´¥: {error_msg}")
                            failed_list.append({"url": video_url, "title": title, "error": error_msg})

                    except Exception as e:
                        print(f"[é‡è¯• {idx}/{retry_count}] âœ— å¼‚å¸¸: {str(e)}")
                        page.remove_listener("response", handle_response_retry)
                        failed_list.append({"url": video_url, "title": failed_item.get("title", f"è§†é¢‘ {idx}"), "error": str(e)})

                    await page.wait_for_timeout(self._random_delay(self.VIDEO_INTERVAL))

                print(f"\n[é‡è¯• ç¬¬{retry_round}è½®å®Œæˆ] æœ¬è½®æˆåŠŸ: {retry_count - len(failed_list)} | ä»å¤±è´¥: {len(failed_list)}")

                if not failed_list:
                    print(f"[é‡è¯•] âœ“ æ‰€æœ‰è§†é¢‘å·²æˆåŠŸä¸‹è½½!")
                    break

            # ========== ä¿å­˜å…ƒæ•°æ® ==========
            user_info = {
                "username": username,
                "work_count": work_count,
                "video_count": video_count,
                "non_video_count": non_video_count,
            }
            self._save_user_metadata(user_folder, user_url, user_info, downloaded_videos_info)
            print(f"\n[ä¿¡æ¯] å·²ä¿å­˜å…ƒæ•°æ®åˆ°: {user_folder}")

            browser_manager.keep_alive()

        except Exception as e:
            print(f"\n[é”™è¯¯] {str(e)}")
            yield make_error_event(str(e))
            return

        # ========== å®Œæˆ ==========
        elapsed = round(time.time() - start_time, 1)
        print(f"\n{'='*60}")
        print(f"[å®Œæˆ] ä½œå“: {work_count} | è§†é¢‘: {video_count} | éè§†é¢‘: {non_video_count}")
        print(f"[å®Œæˆ] æ–°ä¸‹è½½: {succeeded_count} | å·²å­˜åœ¨è·³è¿‡: {skipped_count} | å¤±è´¥: {len(failed_list)}")
        print(f"[å®Œæˆ] è€—æ—¶: {elapsed}s")
        print(f"[å®Œæˆ] ä¿å­˜ç›®å½•: {user_folder}")
        print(f"{'='*60}\n")

        yield make_done_event(
            total=video_count,
            work_count=work_count,
            non_video_count=non_video_count,
            succeeded=succeeded_count,
            skipped=skipped_count,
            failed=len(failed_list),
            skipped_videos=failed_list,
            elapsed_time=elapsed,
            folder_path=str(user_folder),
            username=username,
        )

    async def extract_user_video_urls(self, user_url: str, max_scroll: int = 50, interactive: bool = True) -> list[str]:
        """
        ä»æŠ–éŸ³ç”¨æˆ·ä¸»é¡µæå–æ‰€æœ‰è§†é¢‘é“¾æ¥

        æ‰“å¼€ç”¨æˆ·ä¸»é¡µï¼Œåœ¨ class åŒ…å« 'userNewUi' çš„ div ä¸‹æŸ¥æ‰¾æ‰€æœ‰ a æ ‡ç­¾çš„ hrefï¼Œ
        è‡ªåŠ¨å‘ä¸‹æ»šåŠ¨ä»¥åŠ è½½æ›´å¤šè§†é¢‘ã€‚

        Args:
            user_url: æŠ–éŸ³ç”¨æˆ·ä¸»é¡µURL
            max_scroll: æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™æ»šåŠ¨
            interactive: æ˜¯å¦äº¤äº’æ¨¡å¼ï¼ˆCLIæ¨¡å¼ä¸ºTrueï¼ŒAPIæ¨¡å¼ä¸ºFalseï¼‰

        Returns:
            è§†é¢‘URLåˆ—è¡¨
        """
        try:
            from playwright.async_api import async_playwright
        except ImportError:
            raise DownloaderError(url=user_url, message="è¯·å®‰è£… playwright: pip install playwright")

        # å°†ç³»ç»Ÿæµè§ˆå™¨çš„ç™»å½•æ€åŒæ­¥åˆ°ç‹¬ç«‹ profileï¼Œé¿å…æ–‡ä»¶é”å†²çª
        self._sync_native_profile()

        chrome_path = self._get_chrome_path()
        video_urls: list[str] = []

        async with async_playwright() as p:
            launch_options = {
                "user_data_dir": str(self.PROFILE_DIR),
                "headless": False,
                "args": [
                    "--disable-infobars",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-extensions",
                ],
                "viewport": {"width": 1280, "height": 800},
                "ignore_default_args": ["--enable-automation", "--no-sandbox"],
            }

            if chrome_path:
                launch_options["executable_path"] = chrome_path

            print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            context = await p.chromium.launch_persistent_context(**launch_options)
            print(f"[æŠ–éŸ³ä¸‹è½½] æµè§ˆå™¨å·²å¯åŠ¨")

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                await page.add_init_script("""
                    // éšè— webdriver æ ‡è¯†
                    Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                    delete navigator.__proto__.webdriver;

                    // ä¼ªé€ æ’ä»¶
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [
                            { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer' },
                            { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai' },
                            { name: 'Native Client', filename: 'internal-nacl-plugin' }
                        ]
                    });

                    // è¯­è¨€è®¾ç½®
                    Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });

                    // Chrome å¯¹è±¡
                    window.chrome = { runtime: {}, loadTimes: () => {}, csi: () => {} };

                    // éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
                    Object.defineProperty(navigator, 'maxTouchPoints', { get: () => 1 });
                    Object.defineProperty(navigator, 'hardwareConcurrency', { get: () => 8 });
                """)

                # è®¿é—®ç”¨æˆ·ä¸»é¡µ
                await page.goto(user_url, wait_until="domcontentloaded", timeout=60000)
                await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

                if interactive:
                    # äº¤äº’æ¨¡å¼ï¼šç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨å®ŒæˆéªŒè¯
                    loop = asyncio.get_event_loop()
                    await loop.run_in_executor(
                        None,
                        lambda: input("\n>>> æµè§ˆå™¨å·²æ‰“å¼€ï¼Œå¦‚éœ€å®ŒæˆéªŒè¯è¯·åœ¨æµè§ˆå™¨ä¸­æ“ä½œï¼Œå®Œæˆåå›åˆ°æ­¤å¤„æŒ‰ å›è½¦é”® ç»§ç»­...\n")
                    )
                    await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))
                else:
                    # APIæ¨¡å¼ï¼šå¾ªç¯æ£€æµ‹éªŒè¯ç /ç™»å½•ï¼Œç›´åˆ°é¡µé¢æ­£å¸¸åŠ è½½
                    # ç­‰å¾…é¡µé¢ç¨³å®š
                    try:
                        await page.wait_for_load_state("networkidle", timeout=15000)
                    except Exception:
                        pass

                    # å¾ªç¯æ£€æµ‹éªŒè¯ç /ç™»å½•ï¼ˆå¯èƒ½åå¤å‡ºç°ï¼‰
                    auth_ok = await self._wait_and_retry_auth(page, max_retries=10)
                    if not auth_ok:
                        await self._save_debug_info(page, "auth_timeout")
                        raise DownloaderError(url=user_url, message="éªŒè¯ç /ç™»å½•è¶…æ—¶æœªå®Œæˆ")

                    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆæœ€å¤šé‡è¯•5æ¬¡ï¼‰
                    video_links_found = False
                    for load_attempt in range(5):
                        print(f"[æŠ–éŸ³ä¸‹è½½] ç­‰å¾…é¡µé¢åŠ è½½... (ç¬¬{load_attempt+1}æ¬¡)")

                        # ç­‰å¾…ç½‘ç»œç©ºé—²
                        try:
                            await page.wait_for_load_state("networkidle", timeout=30000)
                        except Exception:
                            pass

                        # æ£€æµ‹æ˜¯å¦æœ‰éªŒè¯ç /ç™»å½•
                        has_block, block_type = await self._check_captcha(page)
                        if has_block:
                            print(f"[æŠ–éŸ³ä¸‹è½½] âš ï¸ æ£€æµ‹åˆ° {block_type}ï¼è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆ...")
                            resolved = await self._wait_for_auth_resolved(page, 120)
                            if not resolved:
                                await self._save_debug_info(page, "auth_timeout_2")
                                raise DownloaderError(url=user_url, message="éªŒè¯ç /ç™»å½•è¶…æ—¶æœªå®Œæˆ")
                            # ç­‰å¾…é¡µé¢åˆ·æ–°
                            wait_time = random.uniform(10, 12)
                            print(f"[æŠ–éŸ³ä¸‹è½½] ç­‰å¾…é¡µé¢åˆ·æ–° ({wait_time:.1f}s)...")
                            await page.wait_for_timeout(int(wait_time * 1000))
                            continue

                        # ç­‰å¾…å®é™…è§†é¢‘é“¾æ¥å‡ºç°ï¼ˆä¸åªæ˜¯å®¹å™¨ï¼‰
                        try:
                            await page.wait_for_selector('a[href*="/video/"]', timeout=15000)
                            print(f"[æŠ–éŸ³ä¸‹è½½] âœ“ è§†é¢‘é“¾æ¥å·²åŠ è½½")
                            video_links_found = True
                            break
                        except Exception:
                            # æ£€æŸ¥æ˜¯å¦æœ‰åŠ è½½æŒ‡ç¤ºå™¨
                            loading = await page.query_selector('div[class*="loading"]')
                            if loading:
                                print(f"[æŠ–éŸ³ä¸‹è½½] é¡µé¢æ­£åœ¨åŠ è½½ä¸­ï¼Œç»§ç»­ç­‰å¾…...")
                                await page.wait_for_timeout(5000)
                            else:
                                print(f"[æŠ–éŸ³ä¸‹è½½] æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥ï¼Œç­‰å¾…é¡µé¢ç»§ç»­åŠ è½½...")
                                await page.wait_for_timeout(5000)

                    if not video_links_found:
                        # æœ€åä¸€æ¬¡å°è¯• - åˆ·æ–°é¡µé¢
                        print(f"[æŠ–éŸ³ä¸‹è½½] å¤šæ¬¡å°è¯•åä»æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥ï¼Œåˆ·æ–°é¡µé¢é‡è¯•...")
                        await page.reload(wait_until="networkidle", timeout=60000)
                        await page.wait_for_timeout(8000)  # åˆ·æ–°åå¤šç­‰ä¸€ä¼šå„¿
                        try:
                            await page.wait_for_selector('a[href*="/video/"]', timeout=30000)
                            print(f"[æŠ–éŸ³ä¸‹è½½] âœ“ åˆ·æ–°åæ‰¾åˆ°è§†é¢‘é“¾æ¥")
                            video_links_found = True
                        except Exception:
                            await self._save_debug_info(page, "no_videos_after_refresh")
                            raise DownloaderError(url=user_url, message="æ— æ³•åŠ è½½è§†é¢‘åˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é‡æ–°ç™»å½•")

                    # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹æ¸²æŸ“å®Œæˆ
                    await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

                # æå–é“¾æ¥çš„JSè„šæœ¬ï¼ˆæ”¯æŒå¤šç§å®¹å™¨é€‰æ‹©å™¨ï¼‰
                extract_js = '''() => {
                    const links = new Set();

                    // å°è¯•å¤šç§å®¹å™¨é€‰æ‹©å™¨
                    const selectors = [
                        'div[class*="userNewUi"]',
                        'div[class*="user-post"]',
                        'div[class*="video-list"]',
                        'ul[class*="video"]',
                        'main'
                    ];

                    for (const selector of selectors) {
                        const containers = document.querySelectorAll(selector);
                        containers.forEach(container => {
                            const aTags = container.querySelectorAll('a[href]');
                            aTags.forEach(a => {
                                // æ’é™¤åº•éƒ¨æ¨èåŒº
                                if (a.closest('.user-page-footer')) return;
                                if (a.closest('[class*="recommend"]')) return;
                                const href = a.getAttribute('href');
                                if (href && href.includes('/video/')) {
                                    links.add(href);
                                }
                            });
                        });
                    }

                    return Array.from(links);
                }'''

                # å®‰å…¨æ‰§è¡Œ evaluateï¼ˆè·¨å¯¼èˆªæ—¶å¯èƒ½å¤±è´¥ï¼‰
                async def safe_evaluate(js: str, default=None):
                    try:
                        return await page.evaluate(js)
                    except Exception:
                        await page.wait_for_timeout(self._random_delay((0.8, 1.5)))
                        try:
                            return await page.evaluate(js)
                        except Exception:
                            return default if default is not None else []

                # æ»šåŠ¨åŠ è½½æ‰€æœ‰è§†é¢‘
                print(f"[æŠ–éŸ³ä¸‹è½½] å¼€å§‹æ»šåŠ¨åŠ è½½è§†é¢‘åˆ—è¡¨...")
                prev_count = 0
                no_change_rounds = 0

                for i in range(max_scroll):
                    hrefs = await safe_evaluate(extract_js)

                    current_count = len(hrefs)
                    if current_count > 0:
                        print(f"[æŠ–éŸ³ä¸‹è½½] æ»šåŠ¨ {i+1}: å·²å‘ç° {current_count} ä¸ªè§†é¢‘é“¾æ¥")

                    if current_count == prev_count:
                        no_change_rounds += 1
                        if no_change_rounds >= 3:
                            print(f"[æŠ–éŸ³ä¸‹è½½] è¿ç»­3æ¬¡æ— æ–°å†…å®¹ï¼Œåœæ­¢æ»šåŠ¨")
                            break
                        # é¡µé¢åŠ è½½æ…¢æ—¶ï¼Œç­‰ä¹…ä¸€ç‚¹å†é‡è¯•
                        print(f"[æŠ–éŸ³ä¸‹è½½] æœªå‘ç°æ–°å†…å®¹ï¼Œç­‰å¾…é¡µé¢åŠ è½½ ({no_change_rounds}/3)...")
                        await page.wait_for_timeout(self._random_delay(self.SCROLL_RETRY_DELAY))
                        continue
                    else:
                        no_change_rounds = 0
                    prev_count = current_count

                    # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æ»šåŠ¨ - å…ˆå°†é¼ æ ‡ç§»åˆ°é¡µé¢ä¸­å¤®ï¼Œå†è§¦å‘ wheel äº‹ä»¶
                    await page.mouse.move(random.randint(900, 1100), random.randint(550, 700))
                    delta_y = random.randint(800, 1500)
                    await page.mouse.wheel(0, delta_y)
                    await page.wait_for_timeout(self._random_delay(self.SCROLL_DELAY))

                # æœ€ç»ˆæå–ä¸€æ¬¡
                hrefs = await safe_evaluate(extract_js)
                print(f"[æŠ–éŸ³ä¸‹è½½] æœ€ç»ˆæå–åˆ° {len(hrefs)} ä¸ªé“¾æ¥")

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘ï¼Œä¿å­˜è°ƒè¯•ä¿¡æ¯
                if len(hrefs) == 0:
                    print(f"[æŠ–éŸ³ä¸‹è½½] âš ï¸ æœªæ‰¾åˆ°è§†é¢‘é“¾æ¥")
                    await self._save_debug_info(page, "no_videos")

                    # è°ƒè¯•ï¼šæ‰“å°é¡µé¢ä¸Šæ‰€æœ‰é“¾æ¥
                    all_links = await page.evaluate('''() => {
                        const links = [];
                        document.querySelectorAll('a[href]').forEach(a => {
                            const href = a.getAttribute('href');
                            if (href && !href.startsWith('javascript:')) {
                                links.push(href.substring(0, 80));
                            }
                        });
                        return links.slice(0, 20);  // åªå–å‰20ä¸ª
                    }''')
                    print(f"[æŠ–éŸ³ä¸‹è½½] é¡µé¢é“¾æ¥ç¤ºä¾‹ (å…±{len(all_links)}ä¸ª): {all_links[:5]}")

                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰éªŒè¯ç /ç™»å½•
                    has_block, block_type = await self._check_captcha(page)
                    if has_block:
                        print(f"[æŠ–éŸ³ä¸‹è½½] âš ï¸ é¡µé¢ä»æœ‰ {block_type}ï¼Œè¯·æ‰‹åŠ¨å®ŒæˆéªŒè¯")

                # å°†ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºå®Œæ•´URLï¼Œåªä¿ç•™è§†é¢‘é“¾æ¥
                for href in hrefs:
                    if href.startswith('/video/'):
                        full_url = f"https://www.douyin.com{href}"
                        video_urls.append(full_url)
                    elif 'douyin.com/video/' in href:
                        video_urls.append(href)

                print(f"[æŠ–éŸ³ä¸‹è½½] å…±æå– {len(video_urls)} ä¸ªæœ‰æ•ˆè§†é¢‘é“¾æ¥")

                # ä¿å­˜è§†é¢‘URLåˆ—è¡¨åˆ°è°ƒè¯•æ—¥å¿—
                if video_urls:
                    self._save_video_urls_log(user_url, video_urls)

            finally:
                await context.close()

        return video_urls

    async def extract_user_videos_with_download_urls(
        self,
        user_url: str,
        max_scroll: int = 50,
        on_progress: Optional[callable] = None,
    ) -> list[dict]:
        """
        ä»æŠ–éŸ³ç”¨æˆ·ä¸»é¡µæå–æ‰€æœ‰è§†é¢‘çš„ä¸‹è½½åœ°å€ï¼ˆä¸€æ¬¡æ€§ï¼Œåªå¼€ä¸€æ¬¡æµè§ˆå™¨ï¼‰

        Args:
            user_url: æŠ–éŸ³ç”¨æˆ·ä¸»é¡µURL
            max_scroll: æœ€å¤§æ»šåŠ¨æ¬¡æ•°
            on_progress: è¿›åº¦å›è°ƒ (current, total, video_info)

        Returns:
            è§†é¢‘ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            {
                "url": è§†é¢‘é¡µé¢URL,
                "title": æ ‡é¢˜,
                "author": ä½œè€…,
                "download_url": çœŸå®ä¸‹è½½åœ°å€,
                "thumbnail": å°é¢å›¾,
            }
        """
        try:
            from playwright.async_api import async_playwright
        except ImportError:
            raise DownloaderError(url=user_url, message="è¯·å®‰è£… playwright: pip install playwright")

        self._sync_native_profile()
        chrome_path = self._get_chrome_path()
        results: list[dict] = []

        async with async_playwright() as p:
            launch_options = {
                "user_data_dir": str(self.PROFILE_DIR),
                "headless": False,
                "args": [
                    "--disable-infobars",
                    "--no-first-run",
                    "--no-default-browser-check",
                    "--disable-extensions",
                ],
                "viewport": {"width": 1280, "height": 800},
                "ignore_default_args": ["--enable-automation", "--no-sandbox"],
            }

            if chrome_path:
                launch_options["executable_path"] = chrome_path

            print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            context = await p.chromium.launch_persistent_context(**launch_options)
            print(f"[æŠ–éŸ³ä¸‹è½½] æµè§ˆå™¨å·²å¯åŠ¨")

            try:
                page = context.pages[0] if context.pages else await context.new_page()

                # æ³¨å…¥åæ£€æµ‹è„šæœ¬
                await page.add_init_script("""
                    // éšè— webdriver æ ‡è¯†
                    Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                    delete navigator.__proto__.webdriver;

                    // ä¼ªé€ æ’ä»¶
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [
                            { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer' },
                            { name: 'Chrome PDF Viewer', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai' },
                            { name: 'Native Client', filename: 'internal-nacl-plugin' }
                        ]
                    });

                    // è¯­è¨€è®¾ç½®
                    Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh', 'en'] });

                    // Chrome å¯¹è±¡
                    window.chrome = { runtime: {}, loadTimes: () => {}, csi: () => {} };

                    // éšè—è‡ªåŠ¨åŒ–ç‰¹å¾
                    Object.defineProperty(navigator, 'maxTouchPoints', { get: () => 1 });
                    Object.defineProperty(navigator, 'hardwareConcurrency', { get: () => 8 });
                """)

                # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰è§†é¢‘é¡µé¢é“¾æ¥ ==========
                print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨è®¿é—®ç”¨æˆ·ä¸»é¡µ: {user_url}")
                await page.goto(user_url, wait_until="domcontentloaded", timeout=60000)
                await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

                # ç­‰å¾…é¡µé¢åŠ è½½
                try:
                    await page.wait_for_load_state("networkidle", timeout=30000)
                except Exception:
                    pass
                try:
                    await page.wait_for_selector('div[class*="userNewUi"]', timeout=60000)
                except Exception:
                    pass
                await page.wait_for_timeout(self._random_delay(self.PAGE_LOAD_DELAY))

                # æå–é“¾æ¥çš„JSè„šæœ¬
                extract_js = '''() => {
                    const containers = document.querySelectorAll('div[class*="userNewUi"]');
                    const links = new Set();
                    containers.forEach(container => {
                        const aTags = container.querySelectorAll('a[href]');
                        aTags.forEach(a => {
                            if (a.closest('.user-page-footer')) return;
                            const href = a.getAttribute('href');
                            if (href && href.includes('/video/')) links.add(href);
                        });
                    });
                    return Array.from(links);
                }'''

                # æ»šåŠ¨åŠ è½½æ‰€æœ‰è§†é¢‘
                print(f"[æŠ–éŸ³ä¸‹è½½] æ­£åœ¨æ»šåŠ¨åŠ è½½è§†é¢‘åˆ—è¡¨...")
                prev_count = 0
                no_change_rounds = 0

                for i in range(max_scroll):
                    try:
                        hrefs = await page.evaluate(extract_js)
                    except Exception:
                        await page.wait_for_timeout(self._random_delay((0.8, 1.5)))
                        continue

                    current_count = len(hrefs)
                    print(f"[æŠ–éŸ³ä¸‹è½½] å·²å‘ç° {current_count} ä¸ªè§†é¢‘...")

                    if current_count == prev_count:
                        no_change_rounds += 1
                        if no_change_rounds >= 3:
                            break
                        print(f"[æŠ–éŸ³ä¸‹è½½] æœªå‘ç°æ–°å†…å®¹ï¼Œç­‰å¾…é¡µé¢åŠ è½½ ({no_change_rounds}/3)...")
                        await page.wait_for_timeout(self._random_delay(self.SCROLL_RETRY_DELAY))
                        continue
                    else:
                        no_change_rounds = 0
                    prev_count = current_count

                    # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·æ»šåŠ¨ - å…ˆå°†é¼ æ ‡ç§»åˆ°é¡µé¢ä¸­å¤®ï¼Œå†è§¦å‘ wheel äº‹ä»¶
                    await page.mouse.move(random.randint(900, 1100), random.randint(550, 700))
                    delta_y = random.randint(800, 1500)
                    await page.mouse.wheel(0, delta_y)
                    await page.wait_for_timeout(self._random_delay(self.SCROLL_DELAY))

                # æœ€ç»ˆæå–
                hrefs = await page.evaluate(extract_js)
                video_urls = []
                for href in hrefs:
                    if href.startswith('/video/'):
                        video_urls.append(f"https://www.douyin.com{href}")
                    elif 'douyin.com/video/' in href:
                        video_urls.append(href)

                total = len(video_urls)
                print(f"[æŠ–éŸ³ä¸‹è½½] å…±æ‰¾åˆ° {total} ä¸ªè§†é¢‘ï¼Œå¼€å§‹è·å–ä¸‹è½½åœ°å€...")

                # ä¿å­˜è§†é¢‘URLåˆ—è¡¨åˆ°è°ƒè¯•æ—¥å¿—
                if video_urls:
                    self._save_video_urls_log(user_url, video_urls)

                # ========== ç¬¬äºŒæ­¥ï¼šé€ä¸ªè·å–ä¸‹è½½åœ°å€ ==========
                for idx, video_url in enumerate(video_urls, 1):
                    video_info = {
                        "url": video_url,
                        "title": f"è§†é¢‘ {idx}",
                        "author": None,
                        "download_url": None,
                        "thumbnail": None,
                        "error": None,
                    }

                    try:
                        # è®¾ç½®ç½‘ç»œç›‘å¬æ•è·è§†é¢‘ä¿¡æ¯
                        video_data = {}
                        video_captured = asyncio.Event()

                        async def handle_response(response):
                            nonlocal video_data
                            try:
                                if "aweme/v1/web/aweme/detail" in response.url or "/aweme/detail" in response.url:
                                    if response.status == 200:
                                        data = await response.json()
                                        if data.get("aweme_detail"):
                                            video_data = data["aweme_detail"]
                                            video_captured.set()
                            except Exception:
                                pass

                        page.on("response", handle_response)

                        # è®¿é—®è§†é¢‘é¡µé¢
                        print(f"[æŠ–éŸ³ä¸‹è½½] [{idx}/{total}] è·å–ä¸‹è½½åœ°å€...")
                        await page.goto(video_url, wait_until="domcontentloaded", timeout=30000)

                        # ç­‰å¾…æ•è·
                        try:
                            await asyncio.wait_for(video_captured.wait(), timeout=10)
                        except asyncio.TimeoutError:
                            pass

                        # ç§»é™¤ç›‘å¬å™¨
                        page.remove_listener("response", handle_response)

                        if video_data:
                            # æå–ä¿¡æ¯
                            video_info["title"] = video_data.get("desc", f"è§†é¢‘ {idx}") or f"è§†é¢‘ {idx}"
                            video_info["author"] = video_data.get("author", {}).get("nickname")
                            video_info["thumbnail"] = video_data.get("video", {}).get("cover", {}).get("url_list", [None])[0]

                            # æå–ä¸‹è½½åœ°å€
                            video = video_data.get("video", {})
                            download_url = None

                            # æ–¹æ³•1: play_addr
                            play_addr = video.get("play_addr", {})
                            url_list = play_addr.get("url_list", [])
                            if url_list:
                                download_url = url_list[0].replace("playwm", "play")

                            # æ–¹æ³•2: bit_rate
                            if not download_url:
                                bit_rate = video.get("bit_rate", [])
                                if bit_rate:
                                    sorted_rates = sorted(bit_rate, key=lambda x: x.get("bit_rate", 0), reverse=True)
                                    play_addr = sorted_rates[0].get("play_addr", {})
                                    url_list = play_addr.get("url_list", [])
                                    if url_list:
                                        download_url = url_list[0]

                            video_info["download_url"] = download_url
                        else:
                            video_info["error"] = "æ— æ³•è·å–è§†é¢‘ä¿¡æ¯"

                    except Exception as e:
                        video_info["error"] = str(e)

                    results.append(video_info)

                    # è¿›åº¦å›è°ƒ
                    if on_progress:
                        on_progress(idx, total, video_info)

                    # éšæœºå»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                    await page.wait_for_timeout(self._random_delay(self.VIDEO_INTERVAL))

            finally:
                print(f"[æŠ–éŸ³ä¸‹è½½] å…³é—­æµè§ˆå™¨...")
                await context.close()

        print(f"[æŠ–éŸ³ä¸‹è½½] å®Œæˆï¼æˆåŠŸè·å– {sum(1 for r in results if r.get('download_url'))} ä¸ªä¸‹è½½åœ°å€")
        return results

    async def download_audio_only(
        self,
        url: str,
        output_dir: Path,
        progress_callback: Optional[IProgressCallback] = None,
    ) -> DownloadResult:
        """ä¸‹è½½éŸ³é¢‘"""
        return await self.download(url, output_dir, "best", progress_callback)
